"""
Pr√©-labellisation automatique du dataset RSNA avec le mod√®le DenseNet121 entra√Æn√©
Organise les 8403 images en benign/malignant/normal pour r√©entra√Ænement
"""
import os
import sys
import shutil
from pathlib import Path
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from PIL import Image
import cv2

# Importer les custom objects
sys.path.append(str(Path(__file__).parent))
from focal_loss import FocalLoss
from cbam import CBAM

# Configuration
CONFIG = {
    'model_path': './models/densenet121_final.keras',
    'rsna_path': '../datasets/rsna_bitirme',
    'output_path': '../datasets/rsna_labeled',
    'csv_output': '../datasets/rsna_predictions.csv',
    'img_size': (224, 224),
    'batch_size': 32,
    'confidence_threshold': 0.50,  # Seuil abaiss√© pour accepter plus de pr√©dictions
}

CLASS_NAMES = ['benign', 'malignant', 'normal']

def load_model(model_path):
    """Charger le mod√®le avec custom objects"""
    print(f"\nüì¶ Chargement du mod√®le: {model_path}")
    
    custom_objects = {
        'FocalLoss': FocalLoss,
        'CBAM': CBAM,
        'focal_loss_fixed': FocalLoss(gamma=2.0, alpha=0.25)
    }
    
    model = keras.models.load_model(model_path, custom_objects=custom_objects)
    print("‚úÖ Mod√®le charg√© avec succ√®s\n")
    return model

def preprocess_image(img_path, img_size):
    """Pr√©traiter une image comme pendant l'entra√Ænement"""
    img = cv2.imread(str(img_path))
    if img is None:
        return None
    
    # Convertir en RGB
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # Redimensionner
    img = cv2.resize(img, img_size)
    
    # Normaliser [0, 1]
    img = img.astype(np.float32) / 255.0
    
    return img

def predict_batch(model, image_paths, img_size, batch_size=32):
    """Pr√©dire par batch pour optimiser la vitesse"""
    predictions = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = []
        valid_indices = []
        
        for idx, path in enumerate(batch_paths):
            img = preprocess_image(path, img_size)
            if img is not None:
                batch_images.append(img)
                valid_indices.append(idx)
        
        if batch_images:
            batch_array = np.array(batch_images)
            batch_preds = model.predict(batch_array, verbose=0)
            
            # Remplir les pr√©dictions
            pred_idx = 0
            for idx in range(len(batch_paths)):
                if idx in valid_indices:
                    predictions.append(batch_preds[pred_idx])
                    pred_idx += 1
                else:
                    predictions.append(None)
        else:
            predictions.extend([None] * len(batch_paths))
    
    return predictions

def collect_rsna_images(rsna_path):
    """Collecter toutes les images RSNA"""
    print("üîç Collecte des images RSNA...")
    
    rsna_path = Path(rsna_path)
    image_paths = []
    
    for patient_folder in sorted(rsna_path.iterdir()):
        if patient_folder.is_dir():
            # Les images sont directement dans le dossier patient
            for img_file in patient_folder.glob('*.png'):
                image_paths.append(img_file)
    
    print(f"‚úÖ {len(image_paths)} images trouv√©es\n")
    return image_paths

def auto_label_rsna(config):
    """Labelliser automatiquement le dataset RSNA"""
    
    print("="*80)
    print("PR√â-LABELLISATION AUTOMATIQUE DATASET RSNA")
    print("="*80)
    
    # 1. Charger le mod√®le
    model = load_model(config['model_path'])
    
    # 2. Collecter les images
    image_paths = collect_rsna_images(config['rsna_path'])
    
    if not image_paths:
        print("‚ùå Aucune image trouv√©e !")
        return
    
    # 3. Cr√©er la structure de sortie
    output_path = Path(config['output_path'])
    for split in ['train', 'test']:
        for class_name in CLASS_NAMES:
            (output_path / split / class_name).mkdir(parents=True, exist_ok=True)
    
    # Dossier pour images √† v√©rifier
    (output_path / 'to_verify').mkdir(parents=True, exist_ok=True)
    
    # 4. Pr√©dire sur toutes les images
    print(f"üîÆ Pr√©diction sur {len(image_paths)} images...")
    print(f"   Batch size: {config['batch_size']}")
    print(f"   Seuil confiance: {config['confidence_threshold']}\n")
    
    predictions = predict_batch(
        model, 
        image_paths, 
        config['img_size'], 
        config['batch_size']
    )
    
    # 5. Organiser les r√©sultats
    results = []
    stats = {
        'benign': 0,
        'malignant': 0,
        'normal': 0,
        'to_verify': 0,
        'corrupted': 0
    }
    
    print("üìä Organisation des images...")
    for idx, (img_path, pred) in enumerate(zip(image_paths, predictions)):
        if idx % 500 == 0:
            print(f"   Progression: {idx}/{len(image_paths)} images trait√©es ({idx/len(image_paths)*100:.1f}%)")
        
        if pred is None:
            stats['corrupted'] += 1
            continue
        
        # Classe pr√©dite et confiance
        class_idx = np.argmax(pred)
        confidence = pred[class_idx]
        predicted_class = CLASS_NAMES[class_idx]
        
        # Informations patient
        patient_id = img_path.parent.name
        view_type = img_path.stem  # ['LCC'], ['LMLO'], etc.
        
        # D√©cider o√π copier l'image
        if confidence >= config['confidence_threshold']:
            # Split 80/20 train/test
            split = 'train' if np.random.random() < 0.8 else 'test'
            dest_folder = output_path / split / predicted_class
            stats[predicted_class] += 1
        else:
            # Confiance faible -> √† v√©rifier manuellement
            dest_folder = output_path / 'to_verify'
            stats['to_verify'] += 1
        
        # Nouveau nom de fichier
        new_filename = f"{patient_id}_{view_type}_{confidence:.3f}.png"
        dest_path = dest_folder / new_filename
        
        # Copier l'image
        shutil.copy2(img_path, dest_path)
        
        # Enregistrer les r√©sultats
        results.append({
            'original_path': str(img_path),
            'patient_id': patient_id,
            'view_type': view_type,
            'predicted_class': predicted_class,
            'confidence': confidence,
            'prob_benign': pred[0],
            'prob_malignant': pred[1],
            'prob_normal': pred[2],
            'needs_verification': confidence < config['confidence_threshold'],
            'new_path': str(dest_path)
        })
    
    # 6. Sauvegarder le CSV
    df = pd.DataFrame(results)
    df.to_csv(config['csv_output'], index=False)
    
    # 7. Afficher les statistiques
    print("\n" + "="*80)
    print("R√âSULTATS DE LA PR√â-LABELLISATION")
    print("="*80 + "\n")
    
    print(f"üìä R√©partition des pr√©dictions:")
    print(f"   Benign:    {stats['benign']:4d} images ({stats['benign']/len(image_paths)*100:.1f}%)")
    print(f"   Malignant: {stats['malignant']:4d} images ({stats['malignant']/len(image_paths)*100:.1f}%)")
    print(f"   Normal:    {stats['normal']:4d} images ({stats['normal']/len(image_paths)*100:.1f}%)")
    print(f"   √Ä v√©rifier: {stats['to_verify']:4d} images ({stats['to_verify']/len(image_paths)*100:.1f}%)")
    print(f"   Corrompues: {stats['corrupted']:4d} images")
    
    print(f"\nüìÅ Images organis√©es dans: {output_path}")
    print(f"üìã Pr√©dictions sauvegard√©es: {config['csv_output']}")
    
    # Statistiques train/test
    train_count = sum(len(list((output_path / 'train' / c).glob('*.png'))) for c in CLASS_NAMES)
    test_count = sum(len(list((output_path / 'test' / c).glob('*.png'))) for c in CLASS_NAMES)
    
    print(f"\nüì¶ Split train/test:")
    print(f"   Train: {train_count} images ({train_count/(train_count+test_count)*100:.1f}%)")
    print(f"   Test:  {test_count} images ({test_count/(train_count+test_count)*100:.1f}%)")
    
    print("\n" + "="*80)
    print("PROCHAINES √âTAPES")
    print("="*80)
    print(f"\n1Ô∏è‚É£  V√âRIFIER LES PR√âDICTIONS DOUTEUSES ({stats['to_verify']} images)")
    print(f"   üìÇ Dossier: {output_path / 'to_verify'}")
    print(f"   üí° D√©placez manuellement vers train/[benign|malignant|normal]/")
    
    print(f"\n2Ô∏è‚É£  COMBINER AVEC DATASET BUSI ACTUEL")
    print(f"   python merge_datasets.py")
    
    print(f"\n3Ô∏è‚É£  R√âENTRA√éNER SUR DATASET MASSIF (~10K images)")
    print(f"   python train_advanced.py --data ../datasets/merged/")
    
    print("\n" + "="*80 + "\n")
    
    return df, stats

if __name__ == '__main__':
    # V√©rifier que le mod√®le existe
    model_path = Path(CONFIG['model_path'])
    if not model_path.exists():
        print(f"‚ùå Mod√®le non trouv√©: {model_path}")
        print("üí° Entra√Ænez d'abord le mod√®le avec: python train_advanced.py")
        sys.exit(1)
    
    # V√©rifier que le dataset RSNA existe
    rsna_path = Path(CONFIG['rsna_path'])
    if not rsna_path.exists():
        print(f"‚ùå Dataset RSNA non trouv√©: {rsna_path}")
        sys.exit(1)
    
    # Seed pour reproductibilit√© du split train/test
    np.random.seed(42)
    
    # Lancer la pr√©-labellisation
    df, stats = auto_label_rsna(CONFIG)
